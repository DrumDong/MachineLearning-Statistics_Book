{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "머신러닝_08th.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMmXCuNloA2+PjpR/ObuFbe"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHakMULXVGRH",
        "colab_type": "text"
      },
      "source": [
        "# 의사결정나무 모델\n",
        "- 질문에 대한 결정을 통해 데이터를 분해하는 모델\n",
        "- 데이터를 2개 혹은 그 이상의 부분집합으로 분할\n",
        "  > -> 데이터가 균일해지도록 분할\n",
        "  - 분류: 비슷한 범주를 갖고 있는 관측치끼리 모음\n",
        "  - 예측: 비슷한 수치를 갖고 있는 관측치끼리 모음\n",
        "\n",
        "- 뿌리마디, 중간마디, 끝마디로 구성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyloEKVzW1Bc",
        "colab_type": "text"
      },
      "source": [
        "## 이진분할\n",
        "![대체 텍스트](https://www.dodomira.com/wp-content/uploads/2016/04/CART_tree_titanic_survivors_KOR.png)\n",
        "\n",
        "- 총 3번의 이진분할\n",
        "- 최종적으로 나타나는 부분집합이랑 끝마디랑 개수가 같음"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCiB5hgGbVRN",
        "colab_type": "text"
      },
      "source": [
        "## 예측나무 모델(Regression Tree)\n",
        "- Indicator 함수를 활용하여 최종 리프 노드(끝마디)의 관측치들만 반환 \n",
        "- 끝마디의 관측치들을 평균내서 출력"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nuow5dE7fQSN",
        "colab_type": "text"
      },
      "source": [
        "### 예측나무 모델링 프로세스\n",
        "- 데이터를 M개로 분할\n",
        "- 최상의 분할은 비용함수를 최소로 할 때 얻어짐\n",
        "- 각 분할에 속해 있는 y값들의 평균으로 예측했을 때 오류가 최소\n",
        "---\n",
        "- 분할변수(j)와 분할점(s)은 어떻게 결정할까?\n",
        " - ex) x1>=2 : j:1,s:2\n",
        " - ex) x1(2,3),x2(5,6),x3(1,7)\n",
        " > 그리드 서치 개념으로 총 6번을 다 구하면서 결정해 나가는 것 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbcA8UYo1_Um",
        "colab_type": "text"
      },
      "source": [
        "## 분류나무 모델\n",
        "- 분류 모델에서의 비용함수 (불순도 측정)\n",
        "  > Misclassification rate, Gini Index, Cross-entropy\n",
        "  ![대체 텍스트](https://mblogthumb-phinf.pstatic.net/MjAxNzA0MDlfMjk1/MDAxNDkxNjcwNjg2NDcx.s4om5ADvHbqHIE4NX5CFyJkDhYZC_iOdhQD3zflaVhMg.YPeQWFJwhs4bIlB2P-6jsmLTbGZPu8Ab6JzrMAcsWCIg.PNG.samsjang/%EC%BA%A1%EC%B2%98.PNG?type=w2)\n",
        "\n",
        "\n",
        "- 분류변수(j)와 분할점(s)은 어떻게 결정할까?\n",
        " > 회귀방식과 똑같이 모든 경우의 수를 고려하여 비용함수를 최소로 하는 것을 찾음\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13SjlJPg_mvB",
        "colab_type": "text"
      },
      "source": [
        "분할법칙\n",
        "- 분할변수와 분할기준은 목표변수의 분포를 가장 잘 구별해주는 쪽으로 정함\n",
        "- 목표변수의 분포를 잘 구별해주는 측도로 순수도(purity)또는 불순도(impurity)를 정의\n",
        "- 예를 들어 클래스 0과 클래스 1의 비율이 45%와 55%인 노드는 각 클래스의 비율이 90%와 10%인 마디에 비하여 순수도가 낮음(불순도가 높음)\n",
        "- 각 노드에서 분할변수와 분할점의 설정은 불순도의 감소가 최대가 되도록 선택"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fkn3AmlLAGlH",
        "colab_type": "text"
      },
      "source": [
        "### Information Gain - Cross Entropy\n",
        "- Entropy(S)\n",
        "> ![대체 텍스트](https://miro.medium.com/max/1204/1*rrMXmTjQ0qIhU77qCObs2g.jpeg)\n",
        "\n",
        "- Information gain(IG): 특정 변수를 사용했을 때 entropy 감소량\n",
        " -  중요한 변수를 찾는데 유용\n",
        "> ![대체 텍스트](https://qph.fs.quoracdn.net/main-qimg-d1575e7ad623662c42426fe1608d067f.webp)\n",
        "> ![대체 텍스트](https://steemitimages.com/0x0/https://cdn.steemitimages.com/DQmcezrB3btz6GqbCKVPt9nLiZv6pemEn2eq25kFXqssx1H/noname01.png)\n",
        "> ![대체 텍스트](https://d2vlcm61l7u1fs.cloudfront.net/media%2Fde6%2Fde6fca3a-42d1-48c4-a4d8-5ac19045f155%2FphpIML5tL.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFlvQwxbJFMU",
        "colab_type": "text"
      },
      "source": [
        "## 개별 트리 모델의 단점\n",
        "- 계층적 구조로 인해 중간에 에러가 발생하면 다음 단계로 에러가 계속 전파\n",
        "- 학습 데이터의 미세한 변동에도 최종 결과 크게 영향\n",
        "- 적은 개수의 노이즈에도 크게 영향\n",
        "- 나무의 최종노드 개수를 늘리면 과적합 위험(Los Bias, High Variance)\n",
        "\n",
        "---\n",
        "해결방안 -> 랜덤 포레스트"
      ]
    }
  ]
}