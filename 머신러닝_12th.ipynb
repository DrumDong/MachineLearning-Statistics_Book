{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "머신러닝_12th.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP+06OFdMjDZ2nw5Hirq7D5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EIMxtvpPvhd",
        "colab_type": "text"
      },
      "source": [
        "# PCA, 주성분 분석\n",
        "Principal component analysis(PCA)\n",
        "\n",
        "---\n",
        "고차원 데이터\n",
        "- 변수의 수가 많음 -> 불필요한 변수 존재\n",
        "- 시각적으로 표현하기 어려움\n",
        "- 계산 복잡도 증가 -> 모델링 비효율적\n",
        "- 중요한 변수만을 선택 -> 차원축소\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVyW68ypQOSZ",
        "colab_type": "text"
      },
      "source": [
        "## 변수선택/추출을 통한 차원 축소\n",
        "- 변수선택(selection): 분석 목적에 부합하는 소수의 예측변수만을 선택\n",
        " - 장점: 선택한 변수 해석 용이\n",
        " - 단점: 변수간 상관관계 고려 어려움\n",
        "- 변수추출(extraction): 예측변수의 변환을 통해 새로운 변수 추출\n",
        " - 장점: 변수간 상관관계 고려, 일반적으로 변수의 개수를 많이 줄일 수 있음\n",
        " - 단점: 추출된 변수의 해석이 어려움\n",
        "\n",
        "\n",
        "지도학습 변수선택/추출, 비지도학습 변수선택/추출 방식으로 총 4가지\n",
        "- Supervised feature selection: Information gain, Stepwise regression, LASSO, Genetic algorithm\n",
        "- Supervised feature extraction: Partial least squares (PLS)\n",
        "- Unsupervised feature selection: PCA loading\n",
        "- Unsupervised feature extration: **Principal component analysis(PCA)**, Wavelets transforms, Autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01G57y0RRkky",
        "colab_type": "text"
      },
      "source": [
        "## PCA 개요\n",
        "- 고차원 데이터를 효과적으로 분석하기 위한 대표적 분석 기법\n",
        "- 차원축소, 시각화, 군집화, 압축\n",
        "- p개의 변수로 구성된 데이터를 k개의 변수로 구성된 데이터로 요약(기존 변수의 선형조합)\n",
        "- 원래 데이터의 분산을 최대한 보존하는 새로운 축을 찾고, 그 축에 데이터를 사영(Projection) 시키는 기법\n",
        "- 주요 목적\n",
        " - 데이터 차원 축소(n by p -> n by k, where k << p )\n",
        " - 데이터 시각화 및 해석\n",
        "- 일반적으로 PCA는 전체 분석 과정중 초기에 사용\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CF1g1K2WSWZj",
        "colab_type": "text"
      },
      "source": [
        "## PCA 수리적 배경\n",
        "- 공분산(Covariance)의 성질\n",
        " - X를 p개의 변수와 n개의 개체로 구성된 n by p 행렬로 정의할 때 X의 공분산 행렬은 다음과 같음\n",
        "\n",
        " ![대체 텍스트](https://t1.daumcdn.net/cfile/tistory/255AAC3758C530412A)\n",
        " - 공분산 행렬의 대각 성분은 각 변수의 분산과 같으며, 비대각행렬은 대응하는 두 변수의 공분산과 같음(변수 개수: p)\n",
        "\n",
        " ![대체 텍스트](https://t1.daumcdn.net/cfile/blog/26495346552F6CE816)\n",
        "\n",
        "- 사영\n",
        " - 한 벡터 b를 다른 벡터 a에 사영시킨다는 것은 벡터 b로부터 벡터 a에 수직인 점까지의 길이를 가지며 벡터 a와 같은 방향을 갖는 벡터를 찾는다는 것을 의미\n",
        "\n",
        "![대체 텍스트](https://i.imgur.com/h21igrF.png)\n",
        "\n",
        "- 고유값 및 고유벡터\n",
        " - 어떤 행렬 A에 대해 상수 λ와 벡터 x가 다음 식을 만족할 때, λ와 x를 각각 행렬 A의 고유값(eigenvalue) 및 고유벡터(eigenvector)라고 함\n",
        " - 벡터에 행렬을 곱한다는 것은 해당 벡터를 선형변환(linear transformatin)한다는 의미 -> 고유벡터는 이 변환에 의해 방향이 변하지 않는 벡터를 의미"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqot9Nf1vdaX",
        "colab_type": "text"
      },
      "source": [
        "## PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXMzeHQCiHbR",
        "colab_type": "text"
      },
      "source": [
        "## PCA 과정\n",
        "1. 데이터 정규화\n",
        "2. 기존 변수의 Covariance(correlation) matrix 계산\n",
        "3. Covariance(correlation) matrix로부터 eigenvalue 및 이에 해당되는 eigenvector를 계산\n",
        "4. Eigenvalue 및 해당되는 eigenvectors를 순서대로 나열\n",
        " > λ(1) > λ(2) > λ(3) > λ(4) > λ(5) \n",
        " \n",
        " > e(1) > e(2) > e(3) > e(4) > e(5)\n",
        "5. 정렬된 egienvector를 토대로 기존 변수를 변환\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wx1kWTrStWr7",
        "colab_type": "text"
      },
      "source": [
        "## PCA 한계\n",
        "- 주성분 분석의 특징\n",
        " - 공분산 행렬의 고유벡터를 사용하므로 단일 가우시안(봉우리가 하나만 있는) 분포로 추정할 수 있는 데이터에 대해 서로 독립적인 축을 찾는데 사용할 수 있음\n",
        "- 한계점 1\n",
        " - 데이터의 분포가 가우시안이 아니거나 다중 가우시안 자료들에 대해서는 적용하기가 어려움\n",
        " - 대안: 커널 PCA, LLE(Locally Linear Embedding)\n",
        "- 한계점 2\n",
        " - 분류/예측 문제에 대해서 데이터의 범주 정보를 고려하지 않기 떄문에 범주간 구분이 잘 되도록 변환을 해주는 것은 아님\n",
        " > 주성분분석은 단순히 변환된 축이 최대 분산방향과 정렬되도록 좌표회전을 수행함.\n",
        " >\n",
        " > 대안: Partial Least Square(PLS)"
      ]
    }
  ]
}