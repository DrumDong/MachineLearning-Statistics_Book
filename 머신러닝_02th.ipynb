{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "머신러닝_02th.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMALrjLYomtRxI0Ha2wi5Pg"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4jq83srgsJN",
        "colab_type": "text"
      },
      "source": [
        "# K-nearest neighbor\n",
        "- 별도의 모델 생성 없이 인접 데이터를 분류/ 예측 하는 방법"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AV2h-rCQgzFZ",
        "colab_type": "text"
      },
      "source": [
        "## nearest neighbor?\n",
        "- 1-nearest neighbor: 가장 가까운 새로운 데이터 이웃 한개를 정의한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tU0PGTBrhDVY",
        "colab_type": "text"
      },
      "source": [
        "## KNN 알고리즘의 구분 및 특징\n",
        "- Instance-based  Learning\n",
        "  - 각각의 관측치(Instance)만을 이용 -> 데이터 예측\n",
        "- Memory - based Learning\n",
        " - 모든 학습 데이터를 메모리에 저장한 후, 이를 바탕으로 예측 시도\n",
        "- Lazy Learning\n",
        "  - 모델을 별도로 학습 X, 테스팅 데이터가 들어와야 작동"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VIEzoE-jpku",
        "colab_type": "text"
      },
      "source": [
        "## KNN 분류모델\n",
        "- 모델이란 말을 붙이기 애매함. ( 모델보다는 방법론 및 알고리즘이라고 표현하는 것이 적절함)\n",
        "- 인접한 k 개의 데이터로부터 majority voting을 시행\n",
        "\n",
        "![대체 텍스트](https://cdn-images-1.medium.com/fit/t/1600/480/1*hncgU7vWLBsRvc8WJhxlkQ.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-egkO7LXsNfK",
        "colab_type": "text"
      },
      "source": [
        "## KNN 예측모델\n",
        "- 이웃들의 Y값을 평균을 낸 값으로 반환(Default)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvZEvw5uMp5Q",
        "colab_type": "text"
      },
      "source": [
        "## KNN 파라미터\n",
        "1. k\n",
        "- 인접한 학습 데이터를 몇 개까지 탐색할 것인가?\n",
        "- 1<= k <= 전체 데이터 개수\n",
        " - k를 매우 작게 할 경우: 데이터의 지역적 틍성을 지나치게 반영 -> Overfitting\n",
        " - k를 너무 크게 할 경우: 다른 범주의 개체를 너무 많이 포함하여 오분류할 위험 -> Underfitting\n",
        "- Classification error를 제일 작게 만들어주는 K개를 찾으면 됨.\n",
        "![대체 텍스트](https://miro.medium.com/max/1986/0*5OA0zZ_L7_IK5F48.png)\n",
        " - k를 작게하면 Train set의 error는 계속 줄어든다. (모든 점의 개별 지역적 특성을 반영하기 때문에) 그래서 Train과 Test를 둘다 최상으로 낮은 지점을 찾아야함.\n",
        "2. Distance Measures\n",
        "- 거리측도(1 - 유사도)\n",
        "- 어떤 Distance를 사용할 것인가?\n",
        "- 데이터 내 변수들이 각기 다른 범위, 분산등을 가질 수 있으므로, 데이터 정규화 및 표준화를 해주는 것이 바람직함."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uL-qChW0P2De",
        "colab_type": "text"
      },
      "source": [
        "### 대표적인 거리척도\n",
        "Euclidean Distance\n",
        "\n",
        "![대체 텍스트](https://kong.re.kr/wp-content/uploads/2018/07/%EC%9C%A0%ED%81%B4%EB%A6%AC%EB%94%94%EC%95%88%EA%B1%B0%EB%A6%AC-1.png)\n",
        " - 2차원: 피타고라스의 법칙 최단거리\n",
        "\n",
        "Manhattan Distance(Taxi cab)\n",
        "- 도시의 다른 지점으로 가기위해 가로질러 가는 것은 사실상 불가능함. 격자형태의 거리이기 때문\n",
        "\n",
        "![대체 텍스트](https://angiogenesis.dkfz.de/oncoexpress/software/cs_clust/dist_004.gif)\n",
        "![대체 텍스트](data:image/png;base64,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)\n",
        "\n",
        "Mahalanobis Distance\n",
        "- 변수 내 분산, 변수 간 공분산을 모두 반영하여 X,Y간 거리를 계산하는 방식\n",
        "- 데이터의 covariance matrix가 identity matrix인 경우는 Euclidean distance와 동일\n",
        "- 상관관계를 고려한 공분산을 사용하는 것이 이 거리공식의 핵심\n",
        "\n",
        "![대체 텍스트](https://lh3.googleusercontent.com/proxy/CV_iWF9IB1vZzW1bMsX7ElBLTRFqJgpG1Drd2RSgkaW47QPlZ316uEYjgK0pkchh5SGCfiQfgNNXxNX8Bi2I0Y_d1UIgDT47TYhcvCCx1LIyJFmMTN7c7o8bwpatYnX63DEolF-FTiDWP8P7iF3anGvdjbtkZ_3ARF8)\n",
        "\n",
        "Correlation Distance\n",
        "- 데이터 간 Pearson correlation을 거리측도로 사용하는 방식으로, 데이터 패턴의 유사도를 반영할 수 있음\n",
        "\n",
        "- 0 <= d<= 2\n",
        "\n",
        "![대체 텍스트](https://i.imgur.com/UqHKLcc.png)\n",
        "\n",
        "- 각각의 수치비교가 아니라 전반적인 패턴의 차이를 보고싶을 때 사용하는 거리\n",
        "\n",
        "Spearman Rank Correlation Distance\n",
        "\n",
        "- 데이터가 순위(Rank)로 구성되어 있을 때 사용\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2oEQs-HXlAS",
        "colab_type": "text"
      },
      "source": [
        "## KNN의 장점\n",
        "- 노이즈에 영향을 크게 받지 X, 특히 마할라노비스 거리와 같이 데이터의 분산을 고려할 경우 더욱 강건.\n",
        "- 데이터 수가 많을 수록 효과적"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoFklogYYrD5",
        "colab_type": "text"
      },
      "source": [
        "## KNN의 단점\n",
        "- 파라미터 k의 값을 설정해야함.\n",
        "- 어떤 거리가 적합한지 불분명, 데이터 특성에 맞는 거리측도를 임의로 선정해야함.\n",
        "- 계산시간이 오래 걸림. (모든 학습 데이터 간의 거리를 계산해야하기 때문에)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVzEPpitZXpH",
        "colab_type": "text"
      },
      "source": [
        "## Weighted Knn 예측 모델\n",
        "- 거리에 따라 다른 점들의 가중치를 다르게 부여하여 예측을 시키는 것.\n",
        "\n",
        "![대체 텍스트](https://i.imgur.com/gLBo1gX.png)\n"
      ]
    }
  ]
}