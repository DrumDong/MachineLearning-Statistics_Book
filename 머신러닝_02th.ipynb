{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "머신러닝_02th.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPlOH6QiAeN2wCPmaGyHlfc"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4jq83srgsJN",
        "colab_type": "text"
      },
      "source": [
        "# K-nearest neighbor\n",
        "- 별도의 모델 생성 없이 인접 데이터를 분류/ 예측 하는 방법"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AV2h-rCQgzFZ",
        "colab_type": "text"
      },
      "source": [
        "## nearest neighbor?\n",
        "- 1-nearest neighbor: 가장 가까운 새로운 데이터 이웃 한개를 정의한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tU0PGTBrhDVY",
        "colab_type": "text"
      },
      "source": [
        "## KNN 알고리즘의 구분 및 특징\n",
        "- Instance-based  Learning\n",
        "  - 각각의 관측치(Instance)만을 이용 -> 데이터 예측\n",
        "- Memory - based Learning\n",
        " - 모든 학습 데이터를 메모리에 저장한 후, 이를 바탕으로 예측 시도\n",
        "- Lazy Learning\n",
        "  - 모델을 별도로 학습 X, 테스팅 데이터가 들어와야 작동"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VIEzoE-jpku",
        "colab_type": "text"
      },
      "source": [
        "## KNN 분류모델\n",
        "- 모델이란 말을 붙이기 애매함. ( 모델보다는 방법론 및 알고리즘이라고 표현하는 것이 적절함)\n",
        "- 인접한 k 개의 데이터로부터 majority voting을 시행\n",
        "\n",
        "![대체 텍스트](https://cdn-images-1.medium.com/fit/t/1600/480/1*hncgU7vWLBsRvc8WJhxlkQ.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-egkO7LXsNfK",
        "colab_type": "text"
      },
      "source": [
        "## KNN 예측모델\n",
        "- 이웃들의 Y값을 평균을 낸 값으로 반환(Default)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvZEvw5uMp5Q",
        "colab_type": "text"
      },
      "source": [
        "## KNN 파라미터\n",
        "1. k\n",
        "- 인접한 학습 데이터를 몇 개까지 탐색할 것인가?\n",
        "- 1<= k <= 전체 데이터 개수\n",
        " - k를 매우 작게 할 경우: 데이터의 지역적 틍성을 지나치게 반영 -> Overfitting\n",
        " - k를 너무 크게 할 경우: 다른 범주의 개체를 너무 많이 포함하여 오분류할 위험 -> Underfitting\n",
        "- Classification error를 제일 작게 만들어주는 K개를 찾으면 됨.\n",
        "![대체 텍스트](https://miro.medium.com/max/1986/0*5OA0zZ_L7_IK5F48.png)\n",
        " - k를 작게하면 Train set의 error는 계속 줄어든다. (모든 점의 개별 지역적 특성을 반영하기 때문에) 그래서 Train과 Test를 둘다 최상으로 낮은 지점을 찾아야함.\n",
        "2. Distance Measures\n",
        "- 거리측도(1 - 유사도)\n",
        "- 어떤 Distance를 사용할 것인가?\n",
        "- 데이터 내 변수들이 각기 다른 범위, 분산등을 가질 수 있으므로, 데이터 정규화 및 표준화를 해주는 것이 바람직함."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uL-qChW0P2De",
        "colab_type": "text"
      },
      "source": [
        "### 대표적인 거리척도\n",
        "\n",
        "---\n",
        "\n",
        "Euclidean Distance\n",
        "\n",
        "![대체 텍스트](https://kong.re.kr/wp-content/uploads/2018/07/%EC%9C%A0%ED%81%B4%EB%A6%AC%EB%94%94%EC%95%88%EA%B1%B0%EB%A6%AC-1.png)\n",
        " - 2차원: 피타고라스의 법칙 최단거리\n",
        "\n",
        "---\n",
        "\n",
        "**Manhattan Distance(Taxi cab)**\n",
        "\n",
        "- 도시의 다른 지점으로 가기위해 가로질러 가는 것은 사실상 불가능함. 격자형태의 거리이기 때문\n",
        "\n",
        "![대체 텍스트](https://angiogenesis.dkfz.de/oncoexpress/software/cs_clust/dist_004.gif)\n",
        "![대체 텍스트](https://img1.daumcdn.net/thumb/R720x0.q80/?scode=mtistory2&fname=http%3A%2F%2Fcfile26.uf.tistory.com%2Fimage%2F216BDB4357542786353917)\n",
        "\n",
        "---\n",
        "\n",
        "**Mahalanobis Distance**\n",
        "\n",
        "- 변수 내 분산, 변수 간 공분산을 모두 반영하여 X,Y간 거리를 계산하는 방식\n",
        "- 데이터의 covariance matrix가 identity matrix인 경우는 Euclidean distance와 동일\n",
        "- 상관관계를 고려한 공분산을 사용하는 것이 이 거리공식의 핵심\n",
        "\n",
        "![대체 텍스트](https://onlinelibrary.wiley.com/cms/asset/03e7935a-1e92-45d4-82d0-8ab0e002abce/maps13314-fig-0005-m.jpg)\n",
        "\n",
        "---\n",
        "\n",
        "**Correlation Distance**\n",
        "\n",
        "- 데이터 간 Pearson correlation을 거리측도로 사용하는 방식으로, 데이터 패턴의 유사도를 반영할 수 있음\n",
        "\n",
        "- 0 <= d<= 2\n",
        "\n",
        "![대체 텍스트](https://i.imgur.com/UqHKLcc.png)\n",
        "\n",
        "- 각각의 수치비교가 아니라 전반적인 패턴의 차이를 보고싶을 때 사용하는 거리\n",
        "\n",
        "Spearman Rank Correlation Distance\n",
        "\n",
        "- 데이터가 순위(Rank)로 구성되어 있을 때 사용\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2oEQs-HXlAS",
        "colab_type": "text"
      },
      "source": [
        "## KNN의 장점\n",
        "- 노이즈에 영향을 크게 받지 X, 특히 마할라노비스 거리와 같이 데이터의 분산을 고려할 경우 더욱 강건.\n",
        "- 데이터 수가 많을 수록 효과적"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoFklogYYrD5",
        "colab_type": "text"
      },
      "source": [
        "## KNN의 단점\n",
        "- 파라미터 k의 값을 설정해야함.\n",
        "- 어떤 거리가 적합한지 불분명, 데이터 특성에 맞는 거리측도를 임의로 선정해야함.\n",
        "- 계산시간이 오래 걸림. (모든 학습 데이터 간의 거리를 계산해야하기 때문에)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVzEPpitZXpH",
        "colab_type": "text"
      },
      "source": [
        "## Weighted Knn 예측 모델\n",
        "- 거리에 따라 다른 점들의 가중치를 다르게 부여하여 예측을 시키는 것.\n",
        "\n",
        "![대체 텍스트](https://i.imgur.com/gLBo1gX.png)\n"
      ]
    }
  ]
}