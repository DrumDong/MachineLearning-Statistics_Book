{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "머신러닝_11th.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMSWSwPIGOgAjSCdKqBEbh2"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJkMJKJ-t_2a",
        "colab_type": "text"
      },
      "source": [
        "# Support Vector Machine(SVM)\n",
        "고차원 데이터셋의 분류 문제에서 좋은 성능을 발휘함. Quadratic programming\n",
        "\n",
        "- SVM은 Training Error를 줄이는 방향으로 모델을 만들면 Testing Error 또한 줄일 수 있음 (Generalization ability)\n",
        "- 통계적 이론에 기반한 모델"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfR9qls9w4Fm",
        "colab_type": "text"
      },
      "source": [
        "## Separating Hyperplane\n",
        "\n",
        "![대체 텍스트](https://miro.medium.com/max/2836/1*FwrX8viaCLljRAAxiSAp8Q.png)\n",
        "\n",
        "- 가장 잘 나누는 hyperplane을 찾아야함.\n",
        " - 어떤 hyperplane이 가장 '좋은' hyperplane인가?\n",
        " - '좋다'는 것의 기준은?\n",
        "\n",
        "Maximizing margin over the traing set = minimizing generalization error = good prediction performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg0e-IOsyenQ",
        "colab_type": "text"
      },
      "source": [
        "## Geometric Margin\n",
        "\n",
        "![대체 텍스트](https://www.researchgate.net/publication/272672243/figure/fig3/AS:615085445758985@1523659206084/Generalized-optimal-separating-hyperplane-this-problem-are-best-addressed-by-allowing.png)\n",
        "\n",
        "- Margin: 각 클래스에서 가장 가까운 관측치 사이의 거리\n",
        "- Margin은 w(기울기) 로 표현 가능\n",
        "\n",
        "---\n",
        "\n",
        "![대체 텍스트](https://www.saedsayad.com/images/SVM_optimize.png)\n",
        "\n",
        "w의 L2 norm이 제곱근을 포함하고 있기 떄문에 계산이 어려움 -> 계산상의 편의를 위해 제곱을 취해줌\n",
        "- Decision variable은 w와 b\n",
        "- Objective function은 separating hyperplane으로 부터 정의된 margin의 역수\n",
        "- Constraint는 training data를 완벽하게 seperating 하는 조건\n",
        "- Objective function(목적식) is quadratic(2차식) and constraint(제약식) is linear -> quatratic programming -> convex optimization -> globally opimal solution exists(전역최적해 존재)\n",
        "- Training data가 linearly separable한 경우에만 해가 존재함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7MMRDlv5y2L",
        "colab_type": "text"
      },
      "source": [
        "## Lagrangian Primal\n",
        "\n",
        "![대체 텍스트](https://image.slidesharecdn.com/supportvectormachine-121112135318-phpapp01/95/support-vector-machine-4-638.jpg?cb=1352729591)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OCafi5_CaYZ",
        "colab_type": "text"
      },
      "source": [
        "##Lagrangian dual\n",
        "![대체 텍스트](https://image.slidesharecdn.com/supportvectormachine-121112135318-phpapp01/95/support-vector-machine-8-638.jpg?cb=1352729591)\n",
        "\n",
        "![대체 텍스트](https://i.stack.imgur.com/mDQfb.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSTarsG5pYNi",
        "colab_type": "text"
      },
      "source": [
        "# Linearly Nonseparable Case (Soft Margin SVM)\n",
        "<-> Hard Margin SVM\n",
        "\n",
        "![대체 텍스트](https://i.pinimg.com/originals/47/01/6f/47016fb4938321248c39ca8be164278c.png)\n",
        "\n",
        "- \"약간의 오차(error)를 허용을 하자\"에서 시작\n",
        "\n",
        "![대체 텍스트](https://qph.fs.quoracdn.net/main-qimg-98b64533b2c242ac9aed0ae5633fb8b4.webp)\n",
        "\n",
        "- Decision variable은 w,b,제타\n",
        "- Slack variable(제타) >= 0 도입하여 training error 허용 -> 그렇다고 마냥 크게 할 수 없음\n",
        "- Objective function에 penalty를 추가하여 억제\n",
        "- C는 margin과 traing error에 대한 trade-off를 결정하는 tuning parameter\n",
        " - C 가 커질수록 : training error를 많이 허용 X -> Overfit\n",
        " - C 가 작아질수록 : training error를 많이 허용 -> underfit\n",
        "-  Traing data가 linear separable하지 않아도 해가  존재함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOakezfI2pOd",
        "colab_type": "text"
      },
      "source": [
        "# Kernel Methods for Nonlinear Classification\n",
        "\n",
        "![대체 텍스트](https://media.springernature.com/full/springer-static/image/art%3A10.1186%2Fs12864-018-5031-0/MediaObjects/12864_2018_5031_Fig1_HTML.png)\n",
        "\n",
        "관측치 x들을 더 높은 차원으로 변환시켜 분류해보자!\n",
        "\n",
        "![대체 텍스트](https://miro.medium.com/max/872/1*zWzeMGyCc7KvGD9X8lwlnQ.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kky2QM27c0m",
        "colab_type": "text"
      },
      "source": [
        "## Mapping Original Space to Kernel Space\n",
        "- SVM을 original space가 아닌 feature space에서 학습\n",
        "- Original space에서 nonlinear decision boundary -> Feature space에서 linear decision boundary\n",
        "- 고차원 feature space에서는 관측치 분류가 더 쉬울 수 있음\n",
        "- 고차원 feature space를 효율적으로 계산할 수 있는 방법이 있음"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bt8pTUSr9mHZ",
        "colab_type": "text"
      },
      "source": [
        "## Kernel Functions\n",
        "\n",
        "![대체 텍스트](https://i.ytimg.com/vi/OmTu0fqUsQk/maxresdefault.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kab9X2qv_stb",
        "colab_type": "text"
      },
      "source": [
        "### Choosing Kernel Functions\n",
        "- SVM 사용시 kernel을 결정하는 것은 어려운 문제 -> 탁히 기준이 없음\n",
        "- 사용하는 kernel에 따라 feature space의 특징이 달라지기 떄문에 데이터의 특성에 맞는 kernel을 결정하는 것은 중요함\n",
        "- 일반적으로는 RBF kernel, sigmoid kernel, low degree polynomial kernel 등이 주로 사용됨"
      ]
    }
  ]
}