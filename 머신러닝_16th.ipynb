{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "머신러닝_16th.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP/xzaO/dfsjxJH3dJAckta"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zRdTNwCnfZ2",
        "colab_type": "text"
      },
      "source": [
        "# KL Divergece\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d18Z5klYn6gx",
        "colab_type": "text"
      },
      "source": [
        "주어진 랜덤 변수에 대한 확률 분포 간의 차이를 정량화하는 것이 바람직하다. 주로 관측된 분포와 실제 분포와의 차이를 계산 할 때 자주 등장하는 개념이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUSZDhFDp-wo",
        "colab_type": "text"
      },
      "source": [
        "이는 Kullback-Leibler Difference(KL difference, KL difference) 또는 상대적 엔트로피와 KL difference의 정규화되고 대칭적인 버전을 제공하는 Jensen-Shannon difference와 같은 정보 이론의 기법을 사용하여 달성할 수 있다. 이러한 채점 방법은 모델링 전 형상 선택을 위한 상호 정보, 여러 가지 다른 분류기 모델의 손실 함수로 사용되는 교차 엔트로피와 같이 널리 사용되는 다른 방법의 계산의 지름길로 사용될 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDLMNkVIo1N-",
        "colab_type": "text"
      },
      "source": [
        "이 노트북에서 알게 될 것들은,\n",
        "- 통계적 거리(Statistical distance)는 랜덤 변수에 대한 다른 확률 분포와 같은 통계적 개체 간의 차이를 계산하는 일반적인 방법이다.\n",
        "- Kullback-Leibler difference는 한 확률 분포와 다른 확률 분포의 차이를 측정하는 점수를 계산한다.\n",
        "- Jensen-Shannon 분산은 KL 분산에서 더 나아가 한 확률 분포의 대칭 점수와 거리 측정을 계산한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0O3dh1uipj-O",
        "colab_type": "text"
      },
      "source": [
        "## 통계적 거리(Statistical Distance)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXqUiYRapxAd",
        "colab_type": "text"
      },
      "source": [
        "두 확률 분포를 비고 하고 싶은 상황은 많다. 특히, 실제 분포와 해당 분포의 근사치 등 변수에 대한 단일 랜덤 변수와 두 가지 다른 확률 분포를 가질 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NL1_JYWSq4PX",
        "colab_type": "text"
      },
      "source": [
        "이 상황에선, 두 분포 사이의 차이를 정량화하는 것이 유용할 수 있다. 일반적으로 이것은 확률분포와 같은 두 통계적 객체 사이의 통계적 거리를 계산하는 문제로 언급된다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTRpVOHiykaT",
        "colab_type": "text"
      },
      "source": [
        "한 가지 접근방식은 두 분포 사이의 거리 측정값을 계산하는 것이다. 이것은 측정치를 해석하기 어려울 수 있기 때문에 어려울 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QFPPuo_1Nh8",
        "colab_type": "text"
      },
      "source": [
        "대신에, 두 분포 사이의 차이를 계산하는 것이 더 흔한 방법이다. 차이점(Divergence)은 척도와 같으나 대칭적이지 않다.(A-B != B-A) 즉, 분포 P와 Q에 대한 차이를 계산하면 Q와 P와 다른 점수를 얻을 수 있는 분포의 차이를 점수화한 것이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdvd-FWsyyPl",
        "colab_type": "text"
      },
      "source": [
        "차이 점수(Divergence scores)는 정 보이론과 더 일반적으로 기계학습에서 많은 다른 계산을 위한 중요한 기초가 된다. 예를 들어, 그들은 분류 모델의 손실 함수로 사용되는 교차 엔트로피와 상호 정보(정보 이득) 같은 점수를 계산하는 지름길을 제공한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5X8IlVoy_pxr",
        "colab_type": "text"
      },
      "source": [
        "차이 점수는 생성 대립 네트워크(GAN) 모델을 최적화할 때 목표 확률 분포를 근사화하는 등 복잡한 모델링 문제를 이해하기 위한 도구로도 직접 사용된다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWaBI1mzA4oT",
        "colab_type": "text"
      },
      "source": [
        "정보 이론으로부터 사용되는 두개의 흔한 차이는 Kullback-Leibler Divergence와 Jensen-Shannon Divergence이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_vBUwogF5sE",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JbnZL5F_xPR",
        "colab_type": "text"
      },
      "source": [
        "## Kullback - Leibler Divergence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQj4szC1BR62",
        "colab_type": "text"
      },
      "source": [
        "KL divergene score 는 하나의 확률분포가 다른 확률분포와 얼마나 다른지 정량화한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2_t4Zj8CXlo",
        "colab_type": "text"
      },
      "source": [
        "두 확률분포 Q와 P 사이의 KL divergence는 이렇게 적히기도 한다.\n",
        "- KL(P || Q)\n",
        "\n",
        "여기서 \"||\" 연산자는 Q로부터의 \"분산\" 또는 Ps 분산을 나타낸다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1-oSV-kEkBr",
        "colab_type": "text"
      },
      "source": [
        "KL Divergence는 P에서 각 사건의 확률에 대한 음의 합계에 P에서 사건의 확률에 대한 Q의 로그에 곱한 값으로 계산할 수 있다.\n",
        "- KL(P || Q) = – sum x in X P(x) * log(Q(x) / P(x))\n",
        "\n",
        "합계 내의 값은 주어진 사건에 대한 차이점이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDf-NwpgFUfs",
        "colab_type": "text"
      },
      "source": [
        "이는 P의 각 사건 확률에 Q의 사건 확률에 대해 P의 사건 확률 일지를 곱한 양의 합과 같다. 이것은 실무에서 더 많이 쓰이는 구현이다.\n",
        "- KL(P || Q) = sum x in X P(x) * log(P(x) / Q(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgvvkfygSoYQ",
        "colab_type": "text"
      },
      "source": [
        "KL divergence 점수에 대한 직관은 P로부터의 사건 확률은 크지만 Q의 동일한 사건 확률은 작을 때 큰 차이가 있다는 것이다. P로부터의 확률은 적고 Q로부터의 확률은 클 때, 역시 큰 차이가 있지만, 첫 번째 사례만큼 크지는 않다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONZ0kHuZS0hv",
        "colab_type": "text"
      },
      "source": [
        "이산형 확률 분포와 연속형 확률 분포 사이의 차이를 측정하는 데 사용할 수 있으며, 후자의 경우 이산형 사건의 확률 합계가 아니라 사건의 적분을 계산한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS87_5YOjQDw",
        "colab_type": "text"
      },
      "source": [
        "KL divergense는 대칭이 아니다.\n",
        "- KL(P || Q) != KL(Q || P)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6MZ0MhoskLQ",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "예시\n",
        "- 세 개의 사건이 있는 랜덤 변수를 다른 색상으로 간주한다. 이 변수에 대해 두 가지 다른 확률 분포를 가질 수 있다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NARs8p3skzf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define distributions\n",
        "events = ['red', 'green', 'blue']\n",
        "p = [0.10, 0.40, 0.50]\n",
        "q = [0.80, 0.15, 0.05]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahcn8UnKs2cN",
        "colab_type": "text"
      },
      "source": [
        "확률 히스토그램으로 직접 비교하기 위해 이러한 확률의 막대 차트를 그릴 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jG34iyfQy1wt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "448ff368-2429-489b-cf7e-11cf658fa1a5"
      },
      "source": [
        "from matplotlib import pyplot\n",
        "# define distributions\n",
        "events = ['red', 'green', 'blue']\n",
        "p = [0.10, 0.40, 0.50]\n",
        "q = [0.80, 0.15, 0.05]\n",
        "print('P=%.3f Q=%.3f' % (sum(p), sum(q)))\n",
        "# plot first distribution\n",
        "pyplot.subplot(2,1,1)\n",
        "pyplot.bar(events, p)\n",
        "# plot second distribution\n",
        "pyplot.subplot(2,1,2)\n",
        "pyplot.bar(events, q)\n",
        "# show the plot\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "P=1.000 Q=1.000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gicTCbFzD81",
        "colab_type": "text"
      },
      "source": [
        "다음 순서로, 두 분포 사이의 KL divergence를 계산하는 함수를 만들 수 있다.\n",
        "- 로그 베이스-2를 사용하여 결과가 비트 단위를 갖도록 하자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2BXM0kqzUVU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate the kl divergence\n",
        "def kl_divergence(p, q):\n",
        "\treturn sum(p[i] * log2(p[i]/q[i]) for i in range(len(p)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6Bm4vwcu3lo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "08d1d7eb-a94f-404e-b254-49069f66e79b"
      },
      "source": [
        "# calculate (P || Q)\n",
        "kl_pq = kl_divergence(p, q)\n",
        "print('KL(P || Q): %.3f bits' % kl_pq)\n",
        "# calculate (Q || P)\n",
        "kl_qp = kl_divergence(q, p)\n",
        "print('KL(Q || P): %.3f bits' % kl_qp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KL(P || Q): 1.927 bits\n",
            "KL(Q || P): 2.022 bits\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQGEOpqatSnr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "b1eaeeca-87b6-44a5-ef69-0abdc3852b26"
      },
      "source": [
        "# example of calculating the kl divergence between two mass functions\n",
        "from math import log2\n",
        " \n",
        "# calculate the kl divergence\n",
        "def kl_divergence(p, q):\n",
        "\treturn sum(p[i] * log2(p[i]/q[i]) for i in range(len(p)))\n",
        " \n",
        "# define distributions\n",
        "p = [0.10, 0.40, 0.50]\n",
        "q = [0.80, 0.15, 0.05]\n",
        "# calculate (P || Q)\n",
        "kl_pq = kl_divergence(p, q)\n",
        "print('KL(P || Q): %.3f bits' % kl_pq)\n",
        "# calculate (Q || P)\n",
        "kl_qp = kl_divergence(q, p)\n",
        "print('KL(Q || P): %.3f bits' % kl_qp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KL(P || Q): 1.927 bits\n",
            "KL(Q || P): 2.022 bits\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS-Vu3pVwF8m",
        "colab_type": "text"
      },
      "source": [
        "Scipy 라이브러리에서는 Kl_div() 함수(위에서 쓴 공식과는 다름)를 제공해준다. 또한,  rel_entr() 함수(위에서 공부한 KL divergence와 같은 공식, \"relative entropy\")를 제공한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gO7DprFzzNmf",
        "colab_type": "text"
      },
      "source": [
        "rel_entr() 함수는 각 확률 분포의 모든 사건에 걸친 확률 list를 인수로 삼고 각 사건에 대한 다이버전스 list를 반환한다. 반환된 list를 합치면 KL의 차이를 알 수 있다. 이 계산은 로그 base-2 대신 자연로그를 사용하므로 단위는 비트 대신 nats로 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQ346lYhx8uV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "19e21425-0f8d-4faf-b601-0eecf08a314a"
      },
      "source": [
        "# example of calculating the kl divergence (relative entropy) with scipy\n",
        "from scipy.special import rel_entr\n",
        "# define distributions\n",
        "p = [0.10, 0.40, 0.50]\n",
        "q = [0.80, 0.15, 0.05]\n",
        "# calculate (P || Q)\n",
        "kl_pq = rel_entr(p, q)\n",
        "print('KL(P || Q): %.3f nats' % sum(kl_pq))\n",
        "# calculate (Q || P)\n",
        "kl_qp = rel_entr(q, p)\n",
        "print('KL(Q || P): %.3f nats' % sum(kl_qp))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KL(P || Q): 1.336 nats\n",
            "KL(Q || P): 1.401 nats\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3UloGmo02Gl",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZ63OwHP3bHD",
        "colab_type": "text"
      },
      "source": [
        "## Jensen-Shannon Divergence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeOxIOWv3da4",
        "colab_type": "text"
      },
      "source": [
        "Jensen-Shannon Divergence는 두 확률분포의 차이를 정량화하는 또다른 방법이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mg1ufKUE4mvz",
        "colab_type": "text"
      },
      "source": [
        "대칭인 정규화된 점수를 계산하기위해 KL divergence를 사용한다. 즉, Q로부터의 P의 차이가 P로부터의 Q와 같다는 것!\n",
        "- JS(P || Q) == JS(Q || P)\n",
        "- JS(P || Q) = 1/2 * KL(P || M) + 1/2 * KL(Q || M)\n",
        "- M = 1/2 * (P + Q)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYckcfi554Hg",
        "colab_type": "text"
      },
      "source": [
        "베이스2 로그를 사용하면 score가 0에서 1사이로 되어, 평탄(smoothed)하고 정규화(normalized)된 KL divergence를 제공하므로, 측정도구로써 유용하다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvPr-mef_Ssw",
        "colab_type": "text"
      },
      "source": [
        "score에 제곱근을 하면 차이(divergence)개념에서 거리(distance)개념으로 바뀜."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWMQor49_b4f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate the kl divergence\n",
        "def kl_divergence(p, q):\n",
        "\treturn sum(p[i] * log2(p[i]/q[i]) for i in range(len(p)))\n",
        " \n",
        "# calculate the js divergence\n",
        "def js_divergence(p, q):\n",
        "\tm = 0.5 *(p + q)\n",
        "\treturn 0.5 * kl_divergence(p, m) + 0.5 * kl_divergence(q, m)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMwmTUFM_e9S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "56a215b6-af6b-4c1a-c35a-3426554666b2"
      },
      "source": [
        "# calculate JS(P || Q)\n",
        "import numpy as np\n",
        "from math import sqrt\n",
        "\n",
        "js_pq = js_divergence(np.array(p), np.array(q))\n",
        "print('JS(P || Q) divergence: %.3f bits' % js_pq)\n",
        "print('JS(P || Q) distance: %.3f' % sqrt(js_pq))\n",
        "\n",
        "js_pq = js_divergence(np.array(q), np.array(p))\n",
        "print('JS(Q || P) divergence: %.3f bits' % js_pq)\n",
        "print('JS(Q || P) distance: %.3f' % sqrt(js_pq))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "JS(P || Q) divergence: 0.420 bits\n",
            "JS(P || Q) distance: 0.648\n",
            "JS(Q || P) divergence: 0.420 bits\n",
            "JS(Q || P) distance: 0.648\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEyEiH55Cc_4",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "정리하자면..\n",
        "- 통계적 거리(Statistical distance)는 랜덤 변수에 대한 다른 확률 분포와 같은 통계적 개체 간의 차이를 계산하는 일반적인 개념이다.\n",
        "- Kullback-Leibler divergence는 하나의 확률분포로부터 다른 확률분포의 차이를 계산하는 점수이다.\n",
        "- Jensen-Shannon divergence는 KL divergence에서 더 나아가, 한 확률 분포의 대칭 점수와 거리 측정을 계산한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-BWPc7ZcgPf",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWmp3kr2cYkh",
        "colab_type": "text"
      },
      "source": [
        "JSD에 기초하여, 계층적 군집분석(hierarchical clustering)을 활용하여 군집 분석을 실행할 수 있음.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5mqOqf7nnEL",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "출처: https://machinelearningmastery.com/divergence-between-probability-distributions/\n",
        "\n",
        "https://www.researchgate.net/publication/221240314_On_Studying_Partial_Coverage_and_Spatial_Clustering_Based_on_Jensen-Shannon_Divergence_in_Sensor_Networks"
      ]
    }
  ]
}